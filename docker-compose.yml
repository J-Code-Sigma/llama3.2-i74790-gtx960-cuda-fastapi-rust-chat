networks:
  appnet:
    driver: bridge

volumes:
  llamacpp-data:


services:
  fastapi:
    build:
      context: ./server/FastAPI
    ports:
      - "8000:8000"
    environment:
      - LLAMACPP_HOST=http://rust-api:8080
      - ENABLE_SCANNERS=true
    depends_on:
      - rust-api
    networks:
      - appnet

  rust-api:
    build:
      context: ./server/RUST
    ports:
      - "8081:8080"
    environment:
      LLAMACPP_HOST: http://llama-cpp:11434
    depends_on:
      - llama-cpp
    networks:
      - appnet
    volumes:
      - ./server/RUST/topics.txt:/app/topics.txt

  llama-cpp:
    build:
      context: ./server/llama-cpp
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

    ports:
      - "11434:11434"
    networks:
      - appnet
    volumes:
      - ./models:/models
      - ./start-llama.sh:/start-llama.sh
    entrypoint: [ "/bin/sh", "/start-llama.sh" ]
